Harnessing Large Language Model and Machine Learning for Patent Application Predictions 
Chi So, Rita Tu 

Our project revolves around leveraging the capabilities of Large Language Models (LLM) and Machine Learning (ML) to streamline the patent application process and provide precise predictions regarding their potential outcomes. Similar to various regulatory submissions, the U.S. patent application procedure demands a substantial allocation of resources across various domains. This encompasses substantial efforts in application preparation, significant time investments for application processing, and the potential for additional time requirements if amendments become necessary. Our primary objective is to furnish patent filers with an economical analytical tool capable of forecasting the probable decision status. This tool will not only offer invaluable insights for optimizing application efficiency but also has the potential to eliminate or reduce the need for post-submission amendments. This will be challenging as the tool will scan for content availability and content uniqueness, both of which can vary greatly from one patent submission to another. In pursuit of this objective, we intend to harness the Harvard USPTO Patent Dataset, an extensive repository of patent applications spanning the years 2004 to 2018[1-2]. We draw inspiration from the successful implementation of DistilRoBERTa-Base in a Stanford study, which achieved 63.32% binary classification accuracy. Our arsenal of algorithms may encompass dataset distillation, BERT-CNN, and K-means clustering to bolster the precision and efficiency of our predictive model[3-5].

References: https://patentdataset.org/ https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_116615529.pdf https://aclanthology.org/2023.acl-short.12.pdf https://arxiv.org/pdf/2004.05150.pdf https://www.sciencedirect.com/science/article/abs/pii/S0172219019300742
